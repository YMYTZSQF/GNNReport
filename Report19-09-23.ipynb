{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have done some experiments combining the embeddings from pytorch biggraph with the GraphRNN framework. I used pytorch biggraph to generate 16-dimensional embedding for each node and I replace the default input of graphRNN with these embeddings.\n",
    "\n",
    "It's default input is the bfs-based adjacent matrix which is $n\\times 40$, where $n$ is the number of nodes. The quality of generated graph is measured by MMD (Maximum Mean Discrepancy), which is the measurement used in graphRNN:\n",
    "$$MMD^2(p||q)=E_{x,y~p}[k(x,y)]+E_{x,y~q}[k(x,y)]-2E_{x~p,y~q}[k(x,y)]$$\n",
    "$$k(x,y) = exp\\frac{W(x,y)}{2\\sigma^{2}}$$\n",
    "$$W(p,q) = inf_{\\gamma \\in \\Pi}E_{(x,y)~\\gamma[||x-y||]}$$\n",
    "where $\\Pi$ is the set of all distributions whose marginals are p and q, and $\\gamma$ is a valid transport plan.\n",
    "\n",
    "Same as what as been measured in graphRNN, I also measured degree and clustering coefficient distributions, as well as average orbit counts statistics\n",
    "\n",
    "_clustering coefficient_ $C_{i}$ for a vertex $v_{i}$ is then given by the proportion of links between the vertices within its neighborhood divided by the number of links that could possibly exist between them.\n",
    "\n",
    "_orbit counts_ the number of occurrences of all orbits with 4 nodes\n",
    "\n",
    "|Method     | Original GraphRNN | GraphRNN with pytorch biggraph embedding |\n",
    "| :---        |    :----:   |          ---: |\n",
    "|Degree     | 0.1631650749      | 1.952898087   |\n",
    "|Clustering coef.   | 0.03183506654       |1.99323512      |\n",
    "|Orbit counts   | 0.06887923342        |1.606267055      |\n",
    "|Memory costs   | 827.75MB       |473.375MB        |\n",
    "\n",
    "The training is on 100 graphs (grids) with size (10\\*10 to 19 \\* 19)\n",
    "The memory costs is reduced due to smaller embedding dimensions (16 vs 40). But the results are much worse than original GraphRNN (the smaller, the better). I have observed the loss does not decrease when using the pytorch biggraph embedding. Next step, I can try larger embedding dimensions and replace the input of the second rnn instead of the first one. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
