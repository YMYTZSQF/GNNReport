
I have read the document of pytorch biggraph. They have given a lot of descriptions about how they split the training set and how they construct negative samples and how to calculate the loss. But they have not specified what model they used to generate the embedding itself. I infered from their descriptions that they are very likely using MLP (Multi-layer perceptron) to generate the embeddings.

To many understanding, their thought is straightforward. The following steps is for the basic scenario without considering different relations(edge types). 

- At first, they have one training graph. All edges are considered as positive examples
- Different points (entities) are divided into groups, then every edge can be grouped by the group number of its two end points, for example, if a edge AB, A is in the group 1 of start point, B is in the group 2 of end point. Then, AB is in the group (1,2). After get these edge groups, for each edge group, they can construct negative samples (aka edges do not exist). For each start point in an edge groups, they randomly sample end points in the same edge group to form edges as negative samples. (It's possible there could be positive samples in these negative samples. But when the graph is big, the author assume that the probability of generating positive samples is low.
- Every time use both positive and negative edges of one edge group to train the MLP, the benefits are that it only needs to load two group of points which the two end points belong to. Hence the memory costs are largely reduced.
- Finally measure the quality of embedding through:

    - Mean Rank: the average of the ranks of all positives (lower is better, best is 1).
    - Mean Reciprocal Rank (MRR): the average of the reciprocal of the ranks of all positives (higher is better, best is 1).
    - Hits@1: the fraction of positives that rank better than all their negatives, i.e., have a rank of 1 (higher is better, best is 1).
    - Hits@10: the fraction of positives that rank in the top 10 among their negatives (higher is better, best is 1).
    - Hits@50: the fraction of positives that rank in the top 50 among their negatives (higher is better, best is 1).
    - Area Under the Curve (AUC): an estimation of the probability that a randomly chosen positive scores higher than a randomly chosen negative (any negative, not only the negatives constructed by corrupting that positive).
    
According to the results reported in their paper, the Hits@10 can achieve about 0.85 in LiveJournal Dataset.

So I think in theory pytorch biggraph is possible to be used to generate large-scale graphs. But since this method is a straight forward method and not designed to generate graphs. So I'm not sure about its performance on generating graphs. 

Next step, I plan to follow some example in the tutorial to get deeper understanding about pytorch biggraph and try to figure out a way to use it to generate graphs in practice.

